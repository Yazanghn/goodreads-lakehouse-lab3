{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66500814-19a2-4e38-ab77-612505f3719c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 14926784\nroot\n |-- book_id: string (nullable = true)\n |-- review_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- author_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- rating: integer (nullable = true)\n |-- review_text: string (nullable = true)\n |-- language_code: string (nullable = true)\n |-- n_votes: long (nullable = true)\n |-- date_added: string (nullable = true)\n |-- review_length_words: integer (nullable = true)\n |-- avg_rating: double (nullable = true)\n |-- num_reviews: long (nullable = true)\n\n+--------+--------------------+--------------------+---------+--------------------+--------------------+------+--------------------+-------------+-------+--------------------+-------------------+------------------+-----------+\n| book_id|           review_id|               title|author_id|                name|             user_id|rating|         review_text|language_code|n_votes|          date_added|review_length_words|        avg_rating|num_reviews|\n+--------+--------------------+--------------------+---------+--------------------+--------------------+------+--------------------+-------------+-------+--------------------+-------------------+------------------+-----------+\n| 2909013|00ab87e137e4c5b45...|              Vanish|  1259390|          Tom Pawlik|61d47f7eb4c29a1ae...|     2|whaaaat did i jus...|          eng|      0|Wed Jul 08 03:31:...|                  5|3.5454545454545454|         22|\n|32703721|0121e1c652ad65d5a...|Not So Little Gre...|  1709139|          Celia Kyle|152890aa867aa36ce...|     4|so far one of the...|          eng|      0|Thu Oct 20 04:54:...|                 35| 4.230769230769231|         26|\n|13539044|0176b401e0bb9c97b...|The Silver Lining...|  1251730|       Matthew Quick|61f95deafc1691367...|     5|great story! i lo...|          eng|      0|Sat Apr 13 05:58:...|                 18| 4.002244668911335|       1782|\n|23480844|030987d1c85083e5f...|              Firsts|  9816754|Laurie Elizabeth ...|6cd0b96677e585f2c...|     4|i couldn't put th...|          eng|      0|Thu Jul 07 21:57:...|                109|3.8419540229885056|        348|\n|   63697|03e595b62b43c96dd...|The Man Who Misto...|   843200|        Oliver Sacks|867d8765c8c0a859b...|     4|the content of th...|        en-US|      0|Sun Jan 24 17:08:...|                 70|3.7316239316239317|        585|\n+--------+--------------------+--------------------+---------+--------------------+--------------------+------+--------------------+-------------+-------+--------------------+-------------------+------------------+-----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# --- Azure storage key setup ---\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.goodreadsreviews60302712.dfs.core.windows.net\",\n",
    "    \"00u5OPJ3GYKrSACO9uvWxs8ZikHiJ7FosLxuXngxN1GqYBs5f2SLr/V0yESwhmfXXSovGYabn6Ai+AStqwR5Tg==\"\n",
    ")\n",
    "\n",
    "# --- Load the features_v1 (from Lab 3) ---\n",
    "spark.sql(\"USE SCHEMA default\")\n",
    "df = spark.table(\"features_v1\")\n",
    "\n",
    "print(\"Total rows:\", df.count())\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28486585-72d1-4a9c-8c80-2327eb5b767f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10450634 Val: 2237780 Test: 2238370\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Basic sanity cleaning\n",
    "df = (\n",
    "    df.filter((F.col(\"rating\") >= 1) & (F.col(\"rating\") <= 5))\n",
    "      .filter(F.length(\"review_text\") >= 10)\n",
    "      .dropna(subset=[\"review_text\", \"rating\"])\n",
    ")\n",
    "\n",
    "# Split the dataset\n",
    "train_df, val_df, test_df = df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "# Save the splits to Gold/features_v2\n",
    "base = \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v2\"\n",
    "(train_df.write.mode(\"overwrite\").format(\"delta\").save(f\"{base}/train\"))\n",
    "(val_df.write.mode(\"overwrite\").format(\"delta\").save(f\"{base}/val\"))\n",
    "(test_df.write.mode(\"overwrite\").format(\"delta\").save(f\"{base}/test\"))\n",
    "\n",
    "print(\"Train:\", train_df.count(), \"Val:\", val_df.count(), \"Test:\", test_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae53ad6d-15cc-442c-b517-a2df8ca8ceac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------------------+-------------------+---------------+\n|           review_id|rating|review_length_words|review_length_chars|review_age_days|\n+--------------------+------+-------------------+-------------------+---------------+\n|16b1077b60e3674e4...|     4|                 65|                357|           NULL|\n|3c604adc11027e462...|     4|                179|                966|           NULL|\n|41ee27142256927ba...|     3|                 16|                 98|           NULL|\n|6589cda6675f6b52b...|     2|                130|                781|           NULL|\n|db7dcf73787f66199...|     4|                 89|                515|           NULL|\n+--------------------+------+-------------------+-------------------+---------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load train split\n",
    "train_df = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v2/train\"\n",
    ")\n",
    "\n",
    "# Normalize text: trim, lowercase, remove control characters\n",
    "def normalize_text(colname):\n",
    "    return F.lower(\n",
    "        F.regexp_replace(\n",
    "            F.trim(F.col(colname)),\n",
    "            r\"[\\x00-\\x1F\\x7F]\", \" \"\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_df = (\n",
    "    train_df\n",
    "    .withColumn(\"review_text\", normalize_text(\"review_text\"))\n",
    "    .withColumn(\"title\", F.trim(F.col(\"title\")))\n",
    "    .withColumn(\"name\", F.trim(F.col(\"name\")))\n",
    "    .filter(F.length(\"review_text\") > 10)\n",
    ")\n",
    "\n",
    "# --- Basic features ---\n",
    "train_df = (\n",
    "    train_df\n",
    "    .withColumn(\"review_length_words\", F.size(F.split(F.col(\"review_text\"), r\"\\s+\")))\n",
    "    .withColumn(\"review_length_chars\", F.length(F.col(\"review_text\")))\n",
    "    .withColumn(\"review_date\", F.to_date(\"date_added\"))\n",
    "    .withColumn(\"review_age_days\", F.datediff(F.current_date(), F.col(\"review_date\")))\n",
    ")\n",
    "\n",
    "# Preview\n",
    "train_df.select(\n",
    "    \"review_id\", \"rating\", \"review_length_words\",\n",
    "    \"review_length_chars\", \"review_age_days\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fea7ac4-64bb-4189-9b55-7e548ecf67f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.write.mode(\"overwrite\").format(\"delta\").save(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v2/train_clean\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d488c5b-e574-4033-90fe-be174f7bee47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: click in /databricks/python3/lib/python3.12/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.12/site-packages (from nltk) (1.4.2)\nCollecting regex>=2021.8.3 (from nltk)\n  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/40.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m20.5/40.5 kB\u001B[0m \u001B[31m653.4 kB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m30.7/40.5 kB\u001B[0m \u001B[31m651.4 kB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.5/40.5 kB\u001B[0m \u001B[31m338.0 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting tqdm (from nltk)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/57.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m51.2/57.7 kB\u001B[0m \u001B[31m1.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.7/57.7 kB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.1/1.5 MB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.1/1.5 MB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.1/1.5 MB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.3/1.5 MB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.3/1.5 MB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.5/1.5 MB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.8/1.5 MB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.8/1.5 MB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m1.3/1.5 MB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/803.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m803.5/803.5 kB\u001B[0m \u001B[31m26.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/78.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.5/78.5 kB\u001B[0m \u001B[31m7.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: tqdm, regex, nltk\nSuccessfully installed nltk-3.9.2 regex-2025.11.3 tqdm-4.67.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /home/spark-\n[nltk_data]     cd8ff534-1416-4f9a-9882-2f/nltk_data...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------+-------------+\n|           review_id|sent_pos|sent_neg|sent_compound|\n+--------------------+--------+--------+-------------+\n|4f761257f88a8ed51...|   0.162|   0.047|       0.8196|\n|78937bbd2d1bc4d68...|   0.132|     0.0|       0.4927|\n|a2acca81bc0462006...|   0.124|   0.097|       0.9509|\n|304c3f0317bdaa18a...|   0.249|   0.031|       0.9783|\n|796ecd157de6b82da...|   0.311|   0.039|       0.9942|\n+--------------------+--------+--------+-------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Install once (on cluster)\n",
    "%pip install nltk\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Initialize analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define UDFs\n",
    "def get_sentiment_score(text, key):\n",
    "    if text is None:\n",
    "        return 0.0\n",
    "    s = sid.polarity_scores(text)\n",
    "    return float(s[key])\n",
    "\n",
    "get_compound = udf(lambda x: get_sentiment_score(x, \"compound\"), DoubleType())\n",
    "get_pos = udf(lambda x: get_sentiment_score(x, \"pos\"), DoubleType())\n",
    "get_neg = udf(lambda x: get_sentiment_score(x, \"neg\"), DoubleType())\n",
    "get_neu = udf(lambda x: get_sentiment_score(x, \"neu\"), DoubleType())\n",
    "\n",
    "# Load train split again\n",
    "train_df = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v2/train_clean\"\n",
    ")\n",
    "\n",
    "# Apply sentiment columns\n",
    "sentiment_df = (\n",
    "    train_df\n",
    "    .withColumn(\"sent_pos\", get_pos(F.col(\"review_text\")))\n",
    "    .withColumn(\"sent_neg\", get_neg(F.col(\"review_text\")))\n",
    "    .withColumn(\"sent_neu\", get_neu(F.col(\"review_text\")))\n",
    "    .withColumn(\"sent_compound\", get_compound(F.col(\"review_text\")))\n",
    ")\n",
    "\n",
    "sentiment_df.select(\"review_id\",\"sent_pos\",\"sent_neg\",\"sent_compound\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d00d63-e586-43f0-adbe-49a097631a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n|           review_id|         review_text|\n+--------------------+--------------------+\n|4f761257f88a8ed51...|...don't really r...|\n|78937bbd2d1bc4d68...|a very good story...|\n|a2acca81bc0462006...|i discovered a co...|\n+--------------------+--------------------+\nonly showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# Load your cleaned training split\n",
    "train_df = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v2/train_clean\"\n",
    ")\n",
    "\n",
    "train_df.select(\"review_id\", \"review_text\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df5646e5-e9fd-448c-860e-644bf7c09e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rows: 10450382\n✅ Full TF-IDF features created and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# STEP 3 – FULL DATASET TF-IDF (SAFE + OPTIMIZED VERSION)\n",
    "# ==============================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import re\n",
    "\n",
    "# --- Optimize Spark for large workloads ---\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark.conf.set(\"spark.default.parallelism\", \"200\")\n",
    "\n",
    "# --- 1️⃣ Load your full cleaned training dataset ---\n",
    "train_df = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v2/train_clean\"\n",
    ")\n",
    "\n",
    "print(\"Training rows:\", train_df.count())\n",
    "\n",
    "# --- 2️⃣ Tokenize text manually (no blocked constructors) ---\n",
    "def simple_tokenize(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    # find all word tokens (alphanumeric)\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "tokenize_udf = F.udf(simple_tokenize, ArrayType(StringType()))\n",
    "tokens_df = train_df.withColumn(\"tokens\", tokenize_udf(F.col(\"review_text\")))\n",
    "\n",
    "# --- 3️⃣ Compute term frequency (TF) per review_id + token ---\n",
    "tf_df = (\n",
    "    tokens_df\n",
    "    .withColumn(\"token\", F.explode(\"tokens\"))\n",
    "    .groupBy(\"review_id\", \"token\")\n",
    "    .agg(F.count(\"*\").alias(\"tf\"))\n",
    ")\n",
    "\n",
    "# --- 4️⃣ Compute document frequency (DF) per token ---\n",
    "df_counts = (\n",
    "    tf_df.select(\"token\", \"review_id\")\n",
    "         .distinct()\n",
    "         .groupBy(\"token\")\n",
    "         .agg(F.count(\"*\").alias(\"df\"))\n",
    ")\n",
    "\n",
    "# --- 5️⃣ Compute inverse document frequency (IDF) ---\n",
    "total_docs = train_df.count()\n",
    "idf_df = df_counts.withColumn(\"idf\", F.log(F.lit(total_docs) / (F.col(\"df\") + 1)))\n",
    "\n",
    "# --- 6️⃣ Join TF + IDF to get TF-IDF per (review_id, token) ---\n",
    "tfidf_df = (\n",
    "    tf_df.join(idf_df, \"token\", \"left\")\n",
    "         .withColumn(\"tfidf\", F.col(\"tf\") * F.col(\"idf\"))\n",
    ")\n",
    "\n",
    "# --- 7️⃣ Aggregate average TF-IDF per review ---\n",
    "agg_tfidf_df = (\n",
    "    tfidf_df.groupBy(\"review_id\")\n",
    "            .agg(F.avg(\"tfidf\").alias(\"avg_tfidf\"))\n",
    ")\n",
    "\n",
    "# --- 8️⃣ Join aggregated TF-IDF back to main training DataFrame ---\n",
    "final_features = (\n",
    "    train_df.join(agg_tfidf_df, \"review_id\", \"left\")\n",
    "            .fillna({\"avg_tfidf\": 0.0})\n",
    ")\n",
    "\n",
    "# --- 9️⃣ Save the new dataset to the Gold layer ---\n",
    "(\n",
    "    final_features\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(\"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v3/train_tfidf_full\")\n",
    ")\n",
    "\n",
    "print(\"✅ Full TF-IDF features created and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34b4f8a2-9427-4035-b7d5-913278800a43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------+\n|review_id                       |avg_tfidf         |\n+--------------------------------+------------------+\n|000250b46f177fe76441bbd178a64671|2.571978419582607 |\n|00027dd587d48a1ea0799f140b0fef60|6.607502762016232 |\n|00064c6ed6be9052c383a5f8a26d1cf9|5.735628913909095 |\n|000671601b00cc04bd8eaf95de8f1ab4|5.2709950740302265|\n|00076d5127e698fa6386a108bf6b3b49|3.7015110192835037|\n|000840728ece4129d2c437e7dbdc4148|4.483717371363774 |\n|0008423c9faf30b7956c58987e4e49b9|5.94937770943427  |\n|000848b3aa2132509689cfbb3f0ccc9a|2.62155774064224  |\n|0008ad99cab8191f3eb09476c74d6e10|4.262934645461822 |\n|0008c54e6ff69a02f01ddc405c1fd8e4|4.154119287532405 |\n+--------------------------------+------------------+\nonly showing top 10 rows\nTotal rows: 10450382\n"
     ]
    }
   ],
   "source": [
    "final_features = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v3/train_tfidf_full\"\n",
    ")\n",
    "final_features.select(\"review_id\", \"avg_tfidf\").show(10, truncate=False)\n",
    "print(\"Total rows:\", final_features.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bd383e-2f3f-4ea7-9947-addb7b0b2ec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded IDF base OK: 10450382\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import re\n",
    "\n",
    "# Load your saved IDF data from Step 3\n",
    "idf_df = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v3/train_tfidf_full\"\n",
    ").select(\"review_id\", \"avg_tfidf\")  # sanity check read works\n",
    "\n",
    "print(\"Loaded IDF base OK:\", idf_df.count())\n",
    "\n",
    "# Function to tokenize safely (same as before)\n",
    "def simple_tokenize(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "tokenize_udf = F.udf(simple_tokenize, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e2c07d-c819-43a7-918b-b5036a843317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation and Test TF-IDF features created successfully!\n"
     ]
    }
   ],
   "source": [
    "# =====================  VALIDATION  =====================\n",
    "val_df = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v2/val\"\n",
    ")\n",
    "\n",
    "tokens_val = val_df.withColumn(\"tokens\", tokenize_udf(F.col(\"review_text\")))\n",
    "\n",
    "# Compute TF for validation\n",
    "tf_val = (\n",
    "    tokens_val\n",
    "    .withColumn(\"token\", F.explode(\"tokens\"))\n",
    "    .groupBy(\"review_id\", \"token\")\n",
    "    .agg(F.count(\"*\").alias(\"tf\"))\n",
    ")\n",
    "\n",
    "# Reuse training IDF weights (computed earlier)\n",
    "total_docs_train = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v2/train_clean\"\n",
    ").count()\n",
    "\n",
    "df_counts_val = (\n",
    "    tf_val.select(\"token\", \"review_id\")\n",
    "          .distinct()\n",
    "          .groupBy(\"token\")\n",
    "          .agg(F.count(\"*\").alias(\"df\"))\n",
    ")\n",
    "\n",
    "idf_val = df_counts_val.withColumn(\n",
    "    \"idf\",\n",
    "    F.log(F.lit(total_docs_train) / (F.col(\"df\") + 1))\n",
    ")\n",
    "\n",
    "tfidf_val = (\n",
    "    tf_val.join(idf_val, \"token\", \"left\")\n",
    "          .withColumn(\"tfidf\", F.col(\"tf\") * F.col(\"idf\"))\n",
    ")\n",
    "\n",
    "agg_val = tfidf_val.groupBy(\"review_id\").agg(F.avg(\"tfidf\").alias(\"avg_tfidf\"))\n",
    "final_val = val_df.join(agg_val, \"review_id\", \"left\").fillna({\"avg_tfidf\": 0.0})\n",
    "\n",
    "final_val.write.mode(\"overwrite\").format(\"delta\").save(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v3/val_tfidf_full\"\n",
    ")\n",
    "\n",
    "# =====================  TEST  =====================\n",
    "test_df = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v2/test\"\n",
    ")\n",
    "\n",
    "tokens_test = test_df.withColumn(\"tokens\", tokenize_udf(F.col(\"review_text\")))\n",
    "\n",
    "tf_test = (\n",
    "    tokens_test\n",
    "    .withColumn(\"token\", F.explode(\"tokens\"))\n",
    "    .groupBy(\"review_id\", \"token\")\n",
    "    .agg(F.count(\"*\").alias(\"tf\"))\n",
    ")\n",
    "\n",
    "df_counts_test = (\n",
    "    tf_test.select(\"token\", \"review_id\")\n",
    "           .distinct()\n",
    "           .groupBy(\"token\")\n",
    "           .agg(F.count(\"*\").alias(\"df\"))\n",
    ")\n",
    "\n",
    "idf_test = df_counts_test.withColumn(\n",
    "    \"idf\",\n",
    "    F.log(F.lit(total_docs_train) / (F.col(\"df\") + 1))\n",
    ")\n",
    "\n",
    "tfidf_test = (\n",
    "    tf_test.join(idf_test, \"token\", \"left\")\n",
    "           .withColumn(\"tfidf\", F.col(\"tf\") * F.col(\"idf\"))\n",
    ")\n",
    "\n",
    "agg_test = tfidf_test.groupBy(\"review_id\").agg(F.avg(\"tfidf\").alias(\"avg_tfidf\"))\n",
    "final_test = test_df.join(agg_test, \"review_id\", \"left\").fillna({\"avg_tfidf\": 0.0})\n",
    "\n",
    "final_test.write.mode(\"overwrite\").format(\"delta\").save(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v3/test_tfidf_full\"\n",
    ")\n",
    "\n",
    "print(\"Validation and Test TF-IDF features created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c1755c-e89c-4c5e-b7e9-e9233f8749d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val rows: 2237751 Test rows: 2238651\n+--------------------------------+------------------+\n|review_id                       |avg_tfidf         |\n+--------------------------------+------------------+\n|000f2a9874877c0bf5d79dbc609fd274|3.667228984818703 |\n|0014695adffdb153acb7ae1e92f1fe68|3.5939799350433574|\n|001670bf56a8c149a50ba794f5df59d6|3.3185408144825224|\n|001aacf065d1f8bd067b0bf511768efa|7.512714756906683 |\n|0023471454a739b777ee495dde3fa018|6.562922043429661 |\n+--------------------------------+------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "check_val = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v3/val_tfidf_full\"\n",
    ")\n",
    "check_test = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v3/test_tfidf_full\"\n",
    ")\n",
    "\n",
    "print(\"Val rows:\", check_val.count(), \"Test rows:\", check_test.count())\n",
    "check_val.select(\"review_id\",\"avg_tfidf\").show(5,truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219b4797-475c-4377-bd77-c5ede2fdd34c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------+-------------------+------------------+\n|review_id                       |rating|review_length_words|avg_tfidf         |\n+--------------------------------+------+-------------------+------------------+\n|000250b46f177fe76441bbd178a64671|4     |32                 |2.571978419582607 |\n|00027dd587d48a1ea0799f140b0fef60|3     |40                 |6.607502762016232 |\n|00064c6ed6be9052c383a5f8a26d1cf9|5     |360                |5.735628913909095 |\n|000671601b00cc04bd8eaf95de8f1ab4|4     |475                |5.2709950740302265|\n|00076d5127e698fa6386a108bf6b3b49|2     |192                |3.7015110192835037|\n+--------------------------------+------+-------------------+------------------+\nonly showing top 5 rows\nRows: 10450382\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "train_final = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60302712.dfs.core.windows.net/gold/features_v3/train_tfidf_full\"\n",
    ")\n",
    "\n",
    "# Optional sanity checks\n",
    "train_final.select(\n",
    "    \"review_id\",\"rating\",\"review_length_words\",\"avg_tfidf\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "print(\"Rows:\", train_final.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9848c17a-a7f2-467a-8a5d-25a0cde32ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab4_Text_Features",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}